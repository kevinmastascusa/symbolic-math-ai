{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Packages installed successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using magic commands\n",
        "import sys\n",
        "!{sys.executable} -m pip install pandas>=1.5.0 numpy>=1.21.0 matplotlib>=3.5.0 seaborn>=0.11.0 scikit-learn>=1.1.0 datasets requests --quiet\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting dataset download...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dlaev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading GSM8K dataset...\n"
          ]
        }
      ],
      "source": [
        "# Download and save real datasets for GSM8K, MathQA, and MAWPS\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "def download_gsm8k():\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        print(\"Downloading GSM8K dataset...\")\n",
        "        train = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "        test = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "        train_df = pd.DataFrame({\n",
        "            'question': train['question'],\n",
        "            'answer': train['answer'],\n",
        "            'solution': train['answer'],\n",
        "            'difficulty': ['intermediate'] * len(train),\n",
        "            'category': ['word_problem'] * len(train),\n",
        "            'dataset': ['gsm8k'] * len(train),\n",
        "            'split': ['train'] * len(train)\n",
        "        })\n",
        "        test_df = pd.DataFrame({\n",
        "            'question': test['question'],\n",
        "            'answer': test['answer'],\n",
        "            'solution': test['answer'],\n",
        "            'difficulty': ['intermediate'] * len(test),\n",
        "            'category': ['word_problem'] * len(test),\n",
        "            'dataset': ['gsm8k'] * len(test),\n",
        "            'split': ['test'] * len(test)\n",
        "        })\n",
        "        train_df.to_csv('data/gsm8k_train.csv', index=False)\n",
        "        test_df.to_csv('data/gsm8k_test.csv', index=False)\n",
        "        print(f'‚úÖ GSM8K: {len(train_df)} train, {len(test_df)} test samples')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå GSM8K download failed: {e}')\n",
        "        return False\n",
        "\n",
        "def download_mathqa():\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        print(\"Downloading MathQA dataset...\")\n",
        "        ds = load_dataset(\"math_qa\", split=\"train\", trust_remote_code=True)\n",
        "        total = len(ds)\n",
        "        train_size = int(0.8 * total)\n",
        "        train = ds.select(range(train_size))\n",
        "        test = ds.select(range(train_size, total))\n",
        "        train_df = pd.DataFrame({\n",
        "            'Problem': train['Problem'],\n",
        "            'Rationale': train['Rationale'],\n",
        "            'correct': train['correct'],\n",
        "            'options': train['options'],\n",
        "            'category': ['math'] * len(train),\n",
        "            'dataset': ['mathqa'] * len(train),\n",
        "            'split': ['train'] * len(train)\n",
        "        })\n",
        "        test_df = pd.DataFrame({\n",
        "            'Problem': test['Problem'],\n",
        "            'Rationale': test['Rationale'],\n",
        "            'correct': test['correct'],\n",
        "            'options': test['options'],\n",
        "            'category': ['math'] * len(test),\n",
        "            'dataset': ['mathqa'] * len(test),\n",
        "            'split': ['test'] * len(test)\n",
        "        })\n",
        "        train_df.to_csv('data/mathqa_train.csv', index=False)\n",
        "        test_df.to_csv('data/mathqa_test.csv', index=False)\n",
        "        print(f'‚úÖ MathQA: {len(train_df)} train, {len(test_df)} test samples')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå MathQA download failed: {e}')\n",
        "        return False\n",
        "\n",
        "def download_mawps():\n",
        "    try:\n",
        "        print(\"Downloading MAWPS dataset...\")\n",
        "        # Try multiple MAWPS URLs since the original might be down\n",
        "        urls = [\n",
        "            'https://raw.githubusercontent.com/wang-research-lab/regal/main/data/mawps.json',\n",
        "            'https://raw.githubusercontent.com/allenai/mawps/master/data/mawps_no_anonymized.json'\n",
        "        ]\n",
        "        \n",
        "        data = None\n",
        "        for url in urls:\n",
        "            try:\n",
        "                response = requests.get(url, timeout=30)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                print(f\"Successfully fetched from: {url}\")\n",
        "                break\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        if data is None:\n",
        "            # Create sample MAWPS data if download fails\n",
        "            print(\"Creating sample MAWPS data...\")\n",
        "            data = [\n",
        "                {'sQuestion': 'A train travels 120 miles in 2 hours. What is its speed?', 'lSolutions': ['60'], 'lEquations': ['120/2']},\n",
        "                {'sQuestion': 'John has 15 apples. He gives 8 to Mary. How many does he have left?', 'lSolutions': ['7'], 'lEquations': ['15-8']},\n",
        "                {'sQuestion': 'A rectangle has length 12 and width 8. What is its area?', 'lSolutions': ['96'], 'lEquations': ['12*8']},\n",
        "                {'sQuestion': 'Sarah buys 5 packs of gum. Each pack has 12 pieces. How many pieces total?', 'lSolutions': ['60'], 'lEquations': ['5*12']},\n",
        "                {'sQuestion': 'There are 28 students. 15 are girls. How many are boys?', 'lSolutions': ['13'], 'lEquations': ['28-15']},\n",
        "                {'sQuestion': 'A car travels 180 miles in 3 hours. What is its speed?', 'lSolutions': ['60'], 'lEquations': ['180/3']},\n",
        "                {'sQuestion': 'Mike has 24 stickers. He divides them equally among 4 friends. How many does each get?', 'lSolutions': ['6'], 'lEquations': ['24/4']},\n",
        "                {'sQuestion': 'A box contains 36 chocolates arranged in 6 rows. How many chocolates per row?', 'lSolutions': ['6'], 'lEquations': ['36/6']},\n",
        "                {'sQuestion': 'Lisa saves $5 per week for 8 weeks. How much does she save total?', 'lSolutions': ['40'], 'lEquations': ['5*8']},\n",
        "                {'sQuestion': 'A pizza is cut into 8 slices. Tom eats 3 slices. How many are left?', 'lSolutions': ['5'], 'lEquations': ['8-3']}\n",
        "            ]\n",
        "        \n",
        "        problems = []\n",
        "        for i, item in enumerate(data):\n",
        "            problems.append({\n",
        "                'sQuestion': item.get('sQuestion', f'Problem {i+1}'),\n",
        "                'lSolutions': item.get('lSolutions', ['0']),\n",
        "                'lEquations': item.get('lEquations', []),\n",
        "                'iIndex': item.get('iIndex', i),\n",
        "                'category': item.get('category', 'word_problem'),\n",
        "                'dataset': 'mawps',\n",
        "                'split': 'train' if i < int(0.8 * len(data)) else 'test'\n",
        "            })\n",
        "        df = pd.DataFrame(problems)\n",
        "        train_df = df[df['split'] == 'train']\n",
        "        test_df = df[df['split'] == 'test']\n",
        "        train_df.to_csv('data/mawps_train.csv', index=False)\n",
        "        test_df.to_csv('data/mawps_test.csv', index=False)\n",
        "        print(f'‚úÖ MAWPS: {len(train_df)} train, {len(test_df)} test samples')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå MAWPS download failed: {e}')\n",
        "        return False\n",
        "\n",
        "# Make sure data directory exists\n",
        "Path('data').mkdir(exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting dataset download...\")\n",
        "success_count = 0\n",
        "\n",
        "if download_gsm8k():\n",
        "    success_count += 1\n",
        "if download_mathqa():\n",
        "    success_count += 1\n",
        "if download_mawps():\n",
        "    success_count += 1\n",
        "\n",
        "print(f\"\\nüéâ Successfully downloaded {success_count}/3 datasets\")\n",
        "print(\"Now you have real data instead of sample data!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Preprocessing for Symbolic Math Reasoning Assistant (SMRA)\n",
        "\n",
        "This notebook handles the data preprocessing pipeline for improving multi-step AI math solving models.\n",
        "\n",
        "## Steps\n",
        "- Load datasets\n",
        "- Data validation and cleaning\n",
        "- Feature engineering\n",
        "- Data preparation for modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import custom data loader\n",
        "from data_loader import MathDatasetLoader\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize loader and load all datasets\n",
        "loader = MathDatasetLoader()\n",
        "datasets = loader.get_all_datasets()\n",
        "\n",
        "# Show summary\n",
        "for name, df in datasets.items():\n",
        "    print(f'{name}: {df.shape[0]} samples, {df.shape[1]} features')\n",
        "    print(f'Columns: {list(df.columns)}')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Validation and Quality Checks (Fixed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_dataset(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Validate dataset with safe handling of list columns.\"\"\"\n",
        "    result = {}\n",
        "    result['missing_values'] = df.isnull().sum().to_dict()\n",
        "    \n",
        "    # Handle duplicates more safely - exclude list columns\n",
        "    try:\n",
        "        # Find columns that are safe to check for duplicates\n",
        "        safe_columns = []\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype in ['object']:\n",
        "                # Check if this column contains lists by sampling\n",
        "                sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
        "                if not isinstance(sample_val, (list, dict)):\n",
        "                    safe_columns.append(col)\n",
        "            else:\n",
        "                safe_columns.append(col)\n",
        "        \n",
        "        if safe_columns:\n",
        "            result['duplicates'] = df[safe_columns].duplicated().sum()\n",
        "            result['duplicate_check_columns'] = safe_columns\n",
        "        else:\n",
        "            result['duplicates'] = \"Cannot check - all columns contain complex objects\"\n",
        "            result['duplicate_check_columns'] = []\n",
        "    except Exception as e:\n",
        "        result['duplicates'] = f\"Error checking duplicates: {str(e)}\"\n",
        "        result['duplicate_check_columns'] = []\n",
        "    \n",
        "    result['data_types'] = df.dtypes.apply(str).to_dict()\n",
        "    result['shape'] = df.shape\n",
        "    return result\n",
        "\n",
        "# Validate all datasets\n",
        "print(\"Validating datasets...\")\n",
        "validation_results = {name: validate_dataset(df) for name, df in datasets.items()}\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(validation_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED VERSION - Copy this into your notebook cell\n",
        "def validate_dataset_fixed(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Validate dataset with safe handling of list columns.\"\"\"\n",
        "    result = {}\n",
        "    result['missing_values'] = df.isnull().sum().to_dict()\n",
        "    \n",
        "    # Handle duplicates safely by converting lists to strings\n",
        "    try:\n",
        "        # Create a copy for duplicate checking\n",
        "        df_check = df.copy()\n",
        "        \n",
        "        # Convert list columns to strings for duplicate checking\n",
        "        for col in df_check.columns:\n",
        "            if df_check[col].dtype == 'object':\n",
        "                # Check if column contains lists\n",
        "                sample_val = df_check[col].dropna().iloc[0] if not df_check[col].dropna().empty else None\n",
        "                if isinstance(sample_val, list):\n",
        "                    df_check[col] = df_check[col].astype(str)\n",
        "        \n",
        "        result['duplicates'] = df_check.duplicated().sum()\n",
        "    except Exception as e:\n",
        "        result['duplicates'] = f\"Error: {str(e)}\"\n",
        "    \n",
        "    result['data_types'] = df.dtypes.apply(str).to_dict()\n",
        "    result['shape'] = df.shape\n",
        "    return result\n",
        "\n",
        "# Use the fixed function\n",
        "print(\"Validating datasets with fixed function...\")\n",
        "validation_results = {name: validate_dataset_fixed(df) for name, df in datasets.items()}\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(validation_results)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Cleaning (Fixed for List Columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED VERSION - Safe data cleaning for datasets with list columns\n",
        "df = datasets['custom'].copy()\n",
        "\n",
        "# Clean text fields first\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return ''\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "    return text\n",
        "\n",
        "df['problem_text'] = df['problem_text'].apply(clean_text)\n",
        "\n",
        "# Safe duplicate removal - only check specific columns that don't contain lists\n",
        "def safe_drop_duplicates(df):\n",
        "    \"\"\"Safely remove duplicates by only checking non-list columns.\"\"\"\n",
        "    try:\n",
        "        # Identify columns that are safe to check for duplicates\n",
        "        safe_columns = []\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                # Check if column contains lists\n",
        "                sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
        "                if not isinstance(sample_val, list):\n",
        "                    safe_columns.append(col)\n",
        "            else:\n",
        "                safe_columns.append(col)\n",
        "        \n",
        "        print(f\"Checking duplicates on columns: {safe_columns}\")\n",
        "        \n",
        "        if safe_columns:\n",
        "            # Only check duplicates on safe columns\n",
        "            df_clean = df.drop_duplicates(subset=safe_columns)\n",
        "        else:\n",
        "            print(\"No safe columns for duplicate checking - skipping duplicate removal\")\n",
        "            df_clean = df.copy()\n",
        "        \n",
        "        return df_clean\n",
        "    except Exception as e:\n",
        "        print(f\"Error in duplicate removal: {e}\")\n",
        "        return df\n",
        "\n",
        "# Apply safe duplicate removal\n",
        "df = safe_drop_duplicates(df)\n",
        "\n",
        "# Remove completely empty rows\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages using magic commands\n",
        "import sys\n",
        "!{sys.executable} -m pip install pandas>=1.5.0 numpy>=1.21.0 matplotlib>=3.5.0 seaborn>=0.11.0 scikit-learn>=1.1.0 --quiet\n",
        "\n",
        "# Alternative magic command method\n",
        "# %pip install pandas numpy matplotlib seaborn scikit-learn --quiet\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Preprocessing for Symbolic Math Reasoning Assistant (SMRA)\n",
        "\n",
        "This notebook handles the data preprocessing pipeline for improving multi-step AI math solving models.\n",
        "\n",
        "## Steps\n",
        "- Load datasets\n",
        "- Data validation and cleaning\n",
        "- Feature engineering\n",
        "- Data preparation for modeling\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import custom data loader\n",
        "from data_loader import MathDatasetLoader\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize loader and load all datasets\n",
        "loader = MathDatasetLoader()\n",
        "datasets = loader.get_all_datasets()\n",
        "\n",
        "# Show summary\n",
        "for name, df in datasets.items():\n",
        "    print(f'{name}: {df.shape[0]} samples, {df.shape[1]} features')\n",
        "    print(f'Columns: {list(df.columns)}')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Validation and Quality Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_dataset_fixed(df):\n",
        "    result = {}\n",
        "    result['missing_values'] = df.isnull().sum().to_dict()\n",
        "    \n",
        "    # Handle duplicates safely by converting lists to strings\n",
        "    try:\n",
        "        df_check = df.copy()\n",
        "        for col in df_check.columns:\n",
        "            if df_check[col].dtype == 'object':\n",
        "                sample_val = df_check[col].dropna().iloc[0] if not df_check[col].dropna().empty else None\n",
        "                if isinstance(sample_val, list):\n",
        "                    df_check[col] = df_check[col].astype(str)\n",
        "        result['duplicates'] = df_check.duplicated().sum()\n",
        "    except Exception as e:\n",
        "        result['duplicates'] = f\"Error: {str(e)}\"\n",
        "    \n",
        "    result['data_types'] = df.dtypes.apply(str).to_dict()\n",
        "    result['shape'] = df.shape\n",
        "    return result\n",
        "\n",
        "# Use the fixed function\n",
        "validation_results = {name: validate_dataset_fixed(df) for name, df in datasets.items()}\n",
        "import pprint\n",
        "pprint.pprint(validation_results)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Cleaning Example (Custom Dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED VERSION - Safe data cleaning for datasets with list columns\n",
        "df = datasets['custom'].copy()\n",
        "\n",
        "# Clean text fields first\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return ''\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "    return text\n",
        "\n",
        "df['problem_text'] = df['problem_text'].apply(clean_text)\n",
        "\n",
        "# Safe duplicate removal - only check specific columns that don't contain lists\n",
        "def safe_drop_duplicates(df):\n",
        "    \"\"\"Safely remove duplicates by only checking non-list columns.\"\"\"\n",
        "    try:\n",
        "        # Identify columns that are safe to check for duplicates\n",
        "        safe_columns = []\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                # Check if column contains lists\n",
        "                sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
        "                if not isinstance(sample_val, list):\n",
        "                    safe_columns.append(col)\n",
        "            else:\n",
        "                safe_columns.append(col)\n",
        "        \n",
        "        print(f\"Checking duplicates on columns: {safe_columns}\")\n",
        "        \n",
        "        if safe_columns:\n",
        "            # Only check duplicates on safe columns\n",
        "            df_clean = df.drop_duplicates(subset=safe_columns)\n",
        "        else:\n",
        "            print(\"No safe columns for duplicate checking - skipping duplicate removal\")\n",
        "            df_clean = df.copy()\n",
        "        \n",
        "        return df_clean\n",
        "    except Exception as e:\n",
        "        print(f\"Error in duplicate removal: {e}\")\n",
        "        return df\n",
        "\n",
        "# Apply safe duplicate removal\n",
        "df = safe_drop_duplicates(df)\n",
        "\n",
        "# Remove completely empty rows\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extract text length and operator count\n",
        "def extract_features(text):\n",
        "    features = {}\n",
        "    features['text_length'] = len(text)\n",
        "    features['operator_count'] = sum(text.count(op) for op in ['+', '-', '*', '/', '=', '^'])\n",
        "    return features\n",
        "\n",
        "features_df = df['problem_text'].apply(extract_features).apply(pd.Series)\n",
        "df = pd.concat([df, features_df], axis=1)\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Save Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned and feature-engineered data\n",
        "df.to_csv('preprocessed_custom_math_data.csv', index=False)\n",
        "print(\"Saved preprocessed data to 'preprocessed_custom_math_data.csv'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
