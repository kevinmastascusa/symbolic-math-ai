{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Packages installed successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using magic commands\n",
        "import sys\n",
        "!{sys.executable} -m pip install pandas>=1.5.0 numpy>=1.21.0 matplotlib>=3.5.0 seaborn>=0.11.0 scikit-learn>=1.1.0 --quiet\n",
        "\n",
        "print(\"✅ Packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Preprocessing for Symbolic Math Reasoning Assistant (SMRA)\n",
        "\n",
        "This notebook handles the data preprocessing pipeline for improving multi-step AI math solving models.\n",
        "\n",
        "## Steps\n",
        "- Load datasets\n",
        "- Data validation and cleaning\n",
        "- Feature engineering\n",
        "- Data preparation for modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import custom data loader\n",
        "from data_loader import MathDatasetLoader\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded GSM8K train dataset from local file\n",
            "Loaded GSM8K test dataset from local file\n",
            "Loaded MathQA train dataset from local file\n",
            "Loaded MathQA test dataset from local file\n",
            "Loaded MAWPS train dataset from local file\n",
            "Loaded MAWPS test dataset from local file\n",
            "Custom dataset not found. Creating sample data...\n",
            "gsm8k_train: 5 samples, 7 features\n",
            "Columns: ['question', 'answer', 'solution', 'difficulty', 'category', 'dataset', 'split']\n",
            "\n",
            "gsm8k_test: 5 samples, 7 features\n",
            "Columns: ['question', 'answer', 'solution', 'difficulty', 'category', 'dataset', 'split']\n",
            "\n",
            "mathqa_train: 5 samples, 7 features\n",
            "Columns: ['Problem', 'Rationale', 'correct', 'options', 'category', 'dataset', 'split']\n",
            "\n",
            "mathqa_test: 5 samples, 7 features\n",
            "Columns: ['Problem', 'Rationale', 'correct', 'options', 'category', 'dataset', 'split']\n",
            "\n",
            "mawps_train: 5 samples, 7 features\n",
            "Columns: ['sQuestion', 'lSolutions', 'lEquations', 'iIndex', 'category', 'dataset', 'split']\n",
            "\n",
            "mawps_test: 5 samples, 7 features\n",
            "Columns: ['sQuestion', 'lSolutions', 'lEquations', 'iIndex', 'category', 'dataset', 'split']\n",
            "\n",
            "custom: 5 samples, 9 features\n",
            "Columns: ['problem_id', 'problem_text', 'difficulty_level', 'subject', 'solution_steps', 'final_answer', 'symbolic_complexity', 'step_count', 'dataset']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize loader and load all datasets\n",
        "loader = MathDatasetLoader()\n",
        "datasets = loader.get_all_datasets()\n",
        "\n",
        "# Show summary\n",
        "for name, df in datasets.items():\n",
        "    print(f'{name}: {df.shape[0]} samples, {df.shape[1]} features')\n",
        "    print(f'Columns: {list(df.columns)}')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Validation and Quality Checks (Fixed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating datasets...\n",
            "{'custom': {'data_types': {'dataset': 'object',\n",
            "                           'difficulty_level': 'object',\n",
            "                           'final_answer': 'object',\n",
            "                           'problem_id': 'object',\n",
            "                           'problem_text': 'object',\n",
            "                           'solution_steps': 'object',\n",
            "                           'step_count': 'int64',\n",
            "                           'subject': 'object',\n",
            "                           'symbolic_complexity': 'int64'},\n",
            "            'duplicate_check_columns': ['problem_id',\n",
            "                                        'problem_text',\n",
            "                                        'difficulty_level',\n",
            "                                        'subject',\n",
            "                                        'final_answer',\n",
            "                                        'symbolic_complexity',\n",
            "                                        'step_count',\n",
            "                                        'dataset'],\n",
            "            'duplicates': 0,\n",
            "            'missing_values': {'dataset': 0,\n",
            "                               'difficulty_level': 0,\n",
            "                               'final_answer': 0,\n",
            "                               'problem_id': 0,\n",
            "                               'problem_text': 0,\n",
            "                               'solution_steps': 0,\n",
            "                               'step_count': 0,\n",
            "                               'subject': 0,\n",
            "                               'symbolic_complexity': 0},\n",
            "            'shape': (5, 9)},\n",
            " 'gsm8k_test': {'data_types': {'answer': 'object',\n",
            "                               'category': 'object',\n",
            "                               'dataset': 'object',\n",
            "                               'difficulty': 'object',\n",
            "                               'question': 'object',\n",
            "                               'solution': 'object',\n",
            "                               'split': 'object'},\n",
            "                'duplicate_check_columns': ['question',\n",
            "                                            'answer',\n",
            "                                            'solution',\n",
            "                                            'difficulty',\n",
            "                                            'category',\n",
            "                                            'dataset',\n",
            "                                            'split'],\n",
            "                'duplicates': 1,\n",
            "                'missing_values': {'answer': 0,\n",
            "                                   'category': 0,\n",
            "                                   'dataset': 0,\n",
            "                                   'difficulty': 0,\n",
            "                                   'question': 0,\n",
            "                                   'solution': 0,\n",
            "                                   'split': 0},\n",
            "                'shape': (5, 7)},\n",
            " 'gsm8k_train': {'data_types': {'answer': 'object',\n",
            "                                'category': 'object',\n",
            "                                'dataset': 'object',\n",
            "                                'difficulty': 'object',\n",
            "                                'question': 'object',\n",
            "                                'solution': 'object',\n",
            "                                'split': 'object'},\n",
            "                 'duplicate_check_columns': ['question',\n",
            "                                             'answer',\n",
            "                                             'solution',\n",
            "                                             'difficulty',\n",
            "                                             'category',\n",
            "                                             'dataset',\n",
            "                                             'split'],\n",
            "                 'duplicates': 1,\n",
            "                 'missing_values': {'answer': 0,\n",
            "                                    'category': 0,\n",
            "                                    'dataset': 0,\n",
            "                                    'difficulty': 0,\n",
            "                                    'question': 0,\n",
            "                                    'solution': 0,\n",
            "                                    'split': 0},\n",
            "                 'shape': (5, 7)},\n",
            " 'mathqa_test': {'data_types': {'Problem': 'object',\n",
            "                                'Rationale': 'object',\n",
            "                                'category': 'object',\n",
            "                                'correct': 'object',\n",
            "                                'dataset': 'object',\n",
            "                                'options': 'object',\n",
            "                                'split': 'object'},\n",
            "                 'duplicate_check_columns': ['Problem',\n",
            "                                             'Rationale',\n",
            "                                             'correct',\n",
            "                                             'options',\n",
            "                                             'category',\n",
            "                                             'dataset',\n",
            "                                             'split'],\n",
            "                 'duplicates': 0,\n",
            "                 'missing_values': {'Problem': 0,\n",
            "                                    'Rationale': 0,\n",
            "                                    'category': 0,\n",
            "                                    'correct': 0,\n",
            "                                    'dataset': 0,\n",
            "                                    'options': 0,\n",
            "                                    'split': 0},\n",
            "                 'shape': (5, 7)},\n",
            " 'mathqa_train': {'data_types': {'Problem': 'object',\n",
            "                                 'Rationale': 'object',\n",
            "                                 'category': 'object',\n",
            "                                 'correct': 'object',\n",
            "                                 'dataset': 'object',\n",
            "                                 'options': 'object',\n",
            "                                 'split': 'object'},\n",
            "                  'duplicate_check_columns': ['Problem',\n",
            "                                              'Rationale',\n",
            "                                              'correct',\n",
            "                                              'options',\n",
            "                                              'category',\n",
            "                                              'dataset',\n",
            "                                              'split'],\n",
            "                  'duplicates': 0,\n",
            "                  'missing_values': {'Problem': 0,\n",
            "                                     'Rationale': 0,\n",
            "                                     'category': 0,\n",
            "                                     'correct': 0,\n",
            "                                     'dataset': 0,\n",
            "                                     'options': 0,\n",
            "                                     'split': 0},\n",
            "                  'shape': (5, 7)},\n",
            " 'mawps_test': {'data_types': {'category': 'object',\n",
            "                               'dataset': 'object',\n",
            "                               'iIndex': 'int64',\n",
            "                               'lEquations': 'object',\n",
            "                               'lSolutions': 'object',\n",
            "                               'sQuestion': 'object',\n",
            "                               'split': 'object'},\n",
            "                'duplicate_check_columns': ['sQuestion',\n",
            "                                            'lSolutions',\n",
            "                                            'lEquations',\n",
            "                                            'iIndex',\n",
            "                                            'category',\n",
            "                                            'dataset',\n",
            "                                            'split'],\n",
            "                'duplicates': 0,\n",
            "                'missing_values': {'category': 0,\n",
            "                                   'dataset': 0,\n",
            "                                   'iIndex': 0,\n",
            "                                   'lEquations': 0,\n",
            "                                   'lSolutions': 0,\n",
            "                                   'sQuestion': 0,\n",
            "                                   'split': 0},\n",
            "                'shape': (5, 7)},\n",
            " 'mawps_train': {'data_types': {'category': 'object',\n",
            "                                'dataset': 'object',\n",
            "                                'iIndex': 'int64',\n",
            "                                'lEquations': 'object',\n",
            "                                'lSolutions': 'object',\n",
            "                                'sQuestion': 'object',\n",
            "                                'split': 'object'},\n",
            "                 'duplicate_check_columns': ['sQuestion',\n",
            "                                             'lSolutions',\n",
            "                                             'lEquations',\n",
            "                                             'iIndex',\n",
            "                                             'category',\n",
            "                                             'dataset',\n",
            "                                             'split'],\n",
            "                 'duplicates': 0,\n",
            "                 'missing_values': {'category': 0,\n",
            "                                    'dataset': 0,\n",
            "                                    'iIndex': 0,\n",
            "                                    'lEquations': 0,\n",
            "                                    'lSolutions': 0,\n",
            "                                    'sQuestion': 0,\n",
            "                                    'split': 0},\n",
            "                 'shape': (5, 7)}}\n"
          ]
        }
      ],
      "source": [
        "def validate_dataset(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Validate dataset with safe handling of list columns.\"\"\"\n",
        "    result = {}\n",
        "    result['missing_values'] = df.isnull().sum().to_dict()\n",
        "    \n",
        "    # Handle duplicates more safely - exclude list columns\n",
        "    try:\n",
        "        # Find columns that are safe to check for duplicates\n",
        "        safe_columns = []\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype in ['object']:\n",
        "                # Check if this column contains lists by sampling\n",
        "                sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
        "                if not isinstance(sample_val, (list, dict)):\n",
        "                    safe_columns.append(col)\n",
        "            else:\n",
        "                safe_columns.append(col)\n",
        "        \n",
        "        if safe_columns:\n",
        "            result['duplicates'] = df[safe_columns].duplicated().sum()\n",
        "            result['duplicate_check_columns'] = safe_columns\n",
        "        else:\n",
        "            result['duplicates'] = \"Cannot check - all columns contain complex objects\"\n",
        "            result['duplicate_check_columns'] = []\n",
        "    except Exception as e:\n",
        "        result['duplicates'] = f\"Error checking duplicates: {str(e)}\"\n",
        "        result['duplicate_check_columns'] = []\n",
        "    \n",
        "    result['data_types'] = df.dtypes.apply(str).to_dict()\n",
        "    result['shape'] = df.shape\n",
        "    return result\n",
        "\n",
        "# Validate all datasets\n",
        "print(\"Validating datasets...\")\n",
        "validation_results = {name: validate_dataset(df) for name, df in datasets.items()}\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(validation_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED VERSION - Copy this into your notebook cell\n",
        "def validate_dataset_fixed(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Validate dataset with safe handling of list columns.\"\"\"\n",
        "    result = {}\n",
        "    result['missing_values'] = df.isnull().sum().to_dict()\n",
        "    \n",
        "    # Handle duplicates safely by converting lists to strings\n",
        "    try:\n",
        "        # Create a copy for duplicate checking\n",
        "        df_check = df.copy()\n",
        "        \n",
        "        # Convert list columns to strings for duplicate checking\n",
        "        for col in df_check.columns:\n",
        "            if df_check[col].dtype == 'object':\n",
        "                # Check if column contains lists\n",
        "                sample_val = df_check[col].dropna().iloc[0] if not df_check[col].dropna().empty else None\n",
        "                if isinstance(sample_val, list):\n",
        "                    df_check[col] = df_check[col].astype(str)\n",
        "        \n",
        "        result['duplicates'] = df_check.duplicated().sum()\n",
        "    except Exception as e:\n",
        "        result['duplicates'] = f\"Error: {str(e)}\"\n",
        "    \n",
        "    result['data_types'] = df.dtypes.apply(str).to_dict()\n",
        "    result['shape'] = df.shape\n",
        "    return result\n",
        "\n",
        "# Use the fixed function\n",
        "print(\"Validating datasets with fixed function...\")\n",
        "validation_results = {name: validate_dataset_fixed(df) for name, df in datasets.items()}\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(validation_results)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Cleaning (Fixed for List Columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED VERSION - Safe data cleaning for datasets with list columns\n",
        "df = datasets['custom'].copy()\n",
        "\n",
        "# Clean text fields first\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return ''\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "    return text\n",
        "\n",
        "df['problem_text'] = df['problem_text'].apply(clean_text)\n",
        "\n",
        "# Safe duplicate removal - only check specific columns that don't contain lists\n",
        "def safe_drop_duplicates(df):\n",
        "    \"\"\"Safely remove duplicates by only checking non-list columns.\"\"\"\n",
        "    try:\n",
        "        # Identify columns that are safe to check for duplicates\n",
        "        safe_columns = []\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                # Check if column contains lists\n",
        "                sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
        "                if not isinstance(sample_val, list):\n",
        "                    safe_columns.append(col)\n",
        "            else:\n",
        "                safe_columns.append(col)\n",
        "        \n",
        "        print(f\"Checking duplicates on columns: {safe_columns}\")\n",
        "        \n",
        "        if safe_columns:\n",
        "            # Only check duplicates on safe columns\n",
        "            df_clean = df.drop_duplicates(subset=safe_columns)\n",
        "        else:\n",
        "            print(\"No safe columns for duplicate checking - skipping duplicate removal\")\n",
        "            df_clean = df.copy()\n",
        "        \n",
        "        return df_clean\n",
        "    except Exception as e:\n",
        "        print(f\"Error in duplicate removal: {e}\")\n",
        "        return df\n",
        "\n",
        "# Apply safe duplicate removal\n",
        "df = safe_drop_duplicates(df)\n",
        "\n",
        "# Remove completely empty rows\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Packages installed successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using magic commands\n",
        "import sys\n",
        "!{sys.executable} -m pip install pandas>=1.5.0 numpy>=1.21.0 matplotlib>=3.5.0 seaborn>=0.11.0 scikit-learn>=1.1.0 --quiet\n",
        "\n",
        "# Alternative magic command method\n",
        "# %pip install pandas numpy matplotlib seaborn scikit-learn --quiet\n",
        "\n",
        "print(\"✅ Packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Preprocessing for Symbolic Math Reasoning Assistant (SMRA)\n",
        "\n",
        "This notebook handles the data preprocessing pipeline for improving multi-step AI math solving models.\n",
        "\n",
        "## Steps\n",
        "- Load datasets\n",
        "- Data validation and cleaning\n",
        "- Feature engineering\n",
        "- Data preparation for modeling\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import custom data loader\n",
        "from data_loader import MathDatasetLoader\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded GSM8K train dataset from local file\n",
            "Loaded GSM8K test dataset from local file\n",
            "Loaded MathQA train dataset from local file\n",
            "Loaded MathQA test dataset from local file\n",
            "Loaded MAWPS train dataset from local file\n",
            "Loaded MAWPS test dataset from local file\n",
            "Custom dataset not found. Creating sample data...\n",
            "gsm8k_train: 5 samples, 7 features\n",
            "Columns: ['question', 'answer', 'solution', 'difficulty', 'category', 'dataset', 'split']\n",
            "\n",
            "gsm8k_test: 5 samples, 7 features\n",
            "Columns: ['question', 'answer', 'solution', 'difficulty', 'category', 'dataset', 'split']\n",
            "\n",
            "mathqa_train: 5 samples, 7 features\n",
            "Columns: ['Problem', 'Rationale', 'correct', 'options', 'category', 'dataset', 'split']\n",
            "\n",
            "mathqa_test: 5 samples, 7 features\n",
            "Columns: ['Problem', 'Rationale', 'correct', 'options', 'category', 'dataset', 'split']\n",
            "\n",
            "mawps_train: 5 samples, 7 features\n",
            "Columns: ['sQuestion', 'lSolutions', 'lEquations', 'iIndex', 'category', 'dataset', 'split']\n",
            "\n",
            "mawps_test: 5 samples, 7 features\n",
            "Columns: ['sQuestion', 'lSolutions', 'lEquations', 'iIndex', 'category', 'dataset', 'split']\n",
            "\n",
            "custom: 5 samples, 9 features\n",
            "Columns: ['problem_id', 'problem_text', 'difficulty_level', 'subject', 'solution_steps', 'final_answer', 'symbolic_complexity', 'step_count', 'dataset']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize loader and load all datasets\n",
        "loader = MathDatasetLoader()\n",
        "datasets = loader.get_all_datasets()\n",
        "\n",
        "# Show summary\n",
        "for name, df in datasets.items():\n",
        "    print(f'{name}: {df.shape[0]} samples, {df.shape[1]} features')\n",
        "    print(f'Columns: {list(df.columns)}')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Validation and Quality Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'custom': {'data_types': {'dataset': 'object',\n",
            "                           'difficulty_level': 'object',\n",
            "                           'final_answer': 'object',\n",
            "                           'problem_id': 'object',\n",
            "                           'problem_text': 'object',\n",
            "                           'solution_steps': 'object',\n",
            "                           'step_count': 'int64',\n",
            "                           'subject': 'object',\n",
            "                           'symbolic_complexity': 'int64'},\n",
            "            'duplicates': 0,\n",
            "            'missing_values': {'dataset': 0,\n",
            "                               'difficulty_level': 0,\n",
            "                               'final_answer': 0,\n",
            "                               'problem_id': 0,\n",
            "                               'problem_text': 0,\n",
            "                               'solution_steps': 0,\n",
            "                               'step_count': 0,\n",
            "                               'subject': 0,\n",
            "                               'symbolic_complexity': 0},\n",
            "            'shape': (5, 9)},\n",
            " 'gsm8k_test': {'data_types': {'answer': 'object',\n",
            "                               'category': 'object',\n",
            "                               'dataset': 'object',\n",
            "                               'difficulty': 'object',\n",
            "                               'question': 'object',\n",
            "                               'solution': 'object',\n",
            "                               'split': 'object'},\n",
            "                'duplicates': 1,\n",
            "                'missing_values': {'answer': 0,\n",
            "                                   'category': 0,\n",
            "                                   'dataset': 0,\n",
            "                                   'difficulty': 0,\n",
            "                                   'question': 0,\n",
            "                                   'solution': 0,\n",
            "                                   'split': 0},\n",
            "                'shape': (5, 7)},\n",
            " 'gsm8k_train': {'data_types': {'answer': 'object',\n",
            "                                'category': 'object',\n",
            "                                'dataset': 'object',\n",
            "                                'difficulty': 'object',\n",
            "                                'question': 'object',\n",
            "                                'solution': 'object',\n",
            "                                'split': 'object'},\n",
            "                 'duplicates': 1,\n",
            "                 'missing_values': {'answer': 0,\n",
            "                                    'category': 0,\n",
            "                                    'dataset': 0,\n",
            "                                    'difficulty': 0,\n",
            "                                    'question': 0,\n",
            "                                    'solution': 0,\n",
            "                                    'split': 0},\n",
            "                 'shape': (5, 7)},\n",
            " 'mathqa_test': {'data_types': {'Problem': 'object',\n",
            "                                'Rationale': 'object',\n",
            "                                'category': 'object',\n",
            "                                'correct': 'object',\n",
            "                                'dataset': 'object',\n",
            "                                'options': 'object',\n",
            "                                'split': 'object'},\n",
            "                 'duplicates': 0,\n",
            "                 'missing_values': {'Problem': 0,\n",
            "                                    'Rationale': 0,\n",
            "                                    'category': 0,\n",
            "                                    'correct': 0,\n",
            "                                    'dataset': 0,\n",
            "                                    'options': 0,\n",
            "                                    'split': 0},\n",
            "                 'shape': (5, 7)},\n",
            " 'mathqa_train': {'data_types': {'Problem': 'object',\n",
            "                                 'Rationale': 'object',\n",
            "                                 'category': 'object',\n",
            "                                 'correct': 'object',\n",
            "                                 'dataset': 'object',\n",
            "                                 'options': 'object',\n",
            "                                 'split': 'object'},\n",
            "                  'duplicates': 0,\n",
            "                  'missing_values': {'Problem': 0,\n",
            "                                     'Rationale': 0,\n",
            "                                     'category': 0,\n",
            "                                     'correct': 0,\n",
            "                                     'dataset': 0,\n",
            "                                     'options': 0,\n",
            "                                     'split': 0},\n",
            "                  'shape': (5, 7)},\n",
            " 'mawps_test': {'data_types': {'category': 'object',\n",
            "                               'dataset': 'object',\n",
            "                               'iIndex': 'int64',\n",
            "                               'lEquations': 'object',\n",
            "                               'lSolutions': 'object',\n",
            "                               'sQuestion': 'object',\n",
            "                               'split': 'object'},\n",
            "                'duplicates': 0,\n",
            "                'missing_values': {'category': 0,\n",
            "                                   'dataset': 0,\n",
            "                                   'iIndex': 0,\n",
            "                                   'lEquations': 0,\n",
            "                                   'lSolutions': 0,\n",
            "                                   'sQuestion': 0,\n",
            "                                   'split': 0},\n",
            "                'shape': (5, 7)},\n",
            " 'mawps_train': {'data_types': {'category': 'object',\n",
            "                                'dataset': 'object',\n",
            "                                'iIndex': 'int64',\n",
            "                                'lEquations': 'object',\n",
            "                                'lSolutions': 'object',\n",
            "                                'sQuestion': 'object',\n",
            "                                'split': 'object'},\n",
            "                 'duplicates': 0,\n",
            "                 'missing_values': {'category': 0,\n",
            "                                    'dataset': 0,\n",
            "                                    'iIndex': 0,\n",
            "                                    'lEquations': 0,\n",
            "                                    'lSolutions': 0,\n",
            "                                    'sQuestion': 0,\n",
            "                                    'split': 0},\n",
            "                 'shape': (5, 7)}}\n"
          ]
        }
      ],
      "source": [
        "def validate_dataset_fixed(df):\n",
        "    result = {}\n",
        "    result['missing_values'] = df.isnull().sum().to_dict()\n",
        "    \n",
        "    # Handle duplicates safely by converting lists to strings\n",
        "    try:\n",
        "        df_check = df.copy()\n",
        "        for col in df_check.columns:\n",
        "            if df_check[col].dtype == 'object':\n",
        "                sample_val = df_check[col].dropna().iloc[0] if not df_check[col].dropna().empty else None\n",
        "                if isinstance(sample_val, list):\n",
        "                    df_check[col] = df_check[col].astype(str)\n",
        "        result['duplicates'] = df_check.duplicated().sum()\n",
        "    except Exception as e:\n",
        "        result['duplicates'] = f\"Error: {str(e)}\"\n",
        "    \n",
        "    result['data_types'] = df.dtypes.apply(str).to_dict()\n",
        "    result['shape'] = df.shape\n",
        "    return result\n",
        "\n",
        "# Use the fixed function\n",
        "validation_results = {name: validate_dataset_fixed(df) for name, df in datasets.items()}\n",
        "import pprint\n",
        "pprint.pprint(validation_results)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Cleaning Example (Custom Dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Remove empty or duplicate rows\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Clean text fields\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n",
            "File \u001b[1;32mc:\\Users\\dlaev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:6825\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6822\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6823\u001b[0m ignore_index \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 6825\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   6826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[0;32m   6827\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
            "File \u001b[1;32mc:\\Users\\dlaev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:6965\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6964\u001b[0m     vals \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[1;32m-> 6965\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   6967\u001b[0m     ids \u001b[38;5;241m=\u001b[39m get_group_index(labels, \u001b[38;5;28mtuple\u001b[39m(shape), sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   6968\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\Users\\dlaev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:6933\u001b[0m, in \u001b[0;36mDataFrame.duplicated.<locals>.f\u001b[1;34m(vals)\u001b[0m\n\u001b[0;32m   6932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(vals) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m-> 6933\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(shape)\n",
            "File \u001b[1;32mc:\\Users\\dlaev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:795\u001b[0m, in \u001b[0;36mfactorize\u001b[1;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[0;32m    793\u001b[0m             values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[1;32m--> 795\u001b[0m     codes, uniques \u001b[38;5;241m=\u001b[39m \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    802\u001b[0m     uniques, codes \u001b[38;5;241m=\u001b[39m safe_sort(\n\u001b[0;32m    803\u001b[0m         uniques,\n\u001b[0;32m    804\u001b[0m         codes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    808\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\dlaev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:595\u001b[0m, in \u001b[0;36mfactorize_array\u001b[1;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    592\u001b[0m hash_klass, values \u001b[38;5;241m=\u001b[39m _get_hashtable_algo(values)\n\u001b[0;32m    594\u001b[0m table \u001b[38;5;241m=\u001b[39m hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m--> 595\u001b[0m uniques, codes \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[0;32m    604\u001b[0m uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n",
            "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7293\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7203\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "# FIXED VERSION - Safe data cleaning for datasets with list columns\n",
        "df = datasets['custom'].copy()\n",
        "\n",
        "# Clean text fields first\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return ''\n",
        "    text = re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "    return text\n",
        "\n",
        "df['problem_text'] = df['problem_text'].apply(clean_text)\n",
        "\n",
        "# Safe duplicate removal - only check specific columns that don't contain lists\n",
        "def safe_drop_duplicates(df):\n",
        "    \"\"\"Safely remove duplicates by only checking non-list columns.\"\"\"\n",
        "    try:\n",
        "        # Identify columns that are safe to check for duplicates\n",
        "        safe_columns = []\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                # Check if column contains lists\n",
        "                sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
        "                if not isinstance(sample_val, list):\n",
        "                    safe_columns.append(col)\n",
        "            else:\n",
        "                safe_columns.append(col)\n",
        "        \n",
        "        print(f\"Checking duplicates on columns: {safe_columns}\")\n",
        "        \n",
        "        if safe_columns:\n",
        "            # Only check duplicates on safe columns\n",
        "            df_clean = df.drop_duplicates(subset=safe_columns)\n",
        "        else:\n",
        "            print(\"No safe columns for duplicate checking - skipping duplicate removal\")\n",
        "            df_clean = df.copy()\n",
        "        \n",
        "        return df_clean\n",
        "    except Exception as e:\n",
        "        print(f\"Error in duplicate removal: {e}\")\n",
        "        return df\n",
        "\n",
        "# Apply safe duplicate removal\n",
        "df = safe_drop_duplicates(df)\n",
        "\n",
        "# Remove completely empty rows\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extract text length and operator count\n",
        "def extract_features(text):\n",
        "    features = {}\n",
        "    features['text_length'] = len(text)\n",
        "    features['operator_count'] = sum(text.count(op) for op in ['+', '-', '*', '/', '=', '^'])\n",
        "    return features\n",
        "\n",
        "features_df = df['problem_text'].apply(extract_features).apply(pd.Series)\n",
        "df = pd.concat([df, features_df], axis=1)\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Save Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned and feature-engineered data\n",
        "df.to_csv('preprocessed_custom_math_data.csv', index=False)\n",
        "print(\"Saved preprocessed data to 'preprocessed_custom_math_data.csv'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
